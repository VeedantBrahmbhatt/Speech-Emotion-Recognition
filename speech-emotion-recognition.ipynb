{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center>                                        Speech Emotion Recognition\n\n### <center>                                    What is Speech Emotion Recognition?\n\n#### Definition : \n\nSpeech emotion recognition (SER) is the field of technology focused on identifying the emotional state of a speaker from their voice.  This goes beyond the words spoken and analyzes  how they are spoken.\n\n#### How it Works :\n\n* **Speech Input**: Similar to standard speech recognition, the user's voice is recorded.\n* **Pre-processing**: The audio is prepared by removing noise and potentially isolating specific speech segments.\n* **Feature Extraction**: Crucial features related to emotions are extracted. These include:\n* **Prosodic features**: Pitch, intonation, volume, speaking rate, pauses\n* **Spectral Features**: Spectrum of the voice, MFCCs (emphasizing qualities similar to human perception)\n* **Voice Quality Features**: Jitter, shimmer (small variations in voice quality)\n* **Emotion Model**: A trained machine learning model (often using classification algorithms) takes these features and identifies the associated emotion.\n* **Emotion Output**: The system outputs the detected emotion, typically with probability or confidence scores (e.g., angry, happy, sad, neutral, etc.).\n\n#### Applications of Speech Emotion Recognition :\n\n* **Mental Health**: Potential uses in diagnosing and monitoring mental health conditions, detecting stress or depression.\n* **Customer Service**: Analyzing customer interactions in call centers to improve service and gauge satisfaction.\n* **Human-Computer Interaction**: Creating more responsive and emotionally intelligent virtual assistants and robots.\n* **Market Research**: Analyzing focus group responses or advertisement reception to understand emotional reactions.\n* **Game Design**: Developing adaptive games that change based on a player's emotional state.","metadata":{}},{"cell_type":"markdown","source":"* **Dataset Used** : Ryerson Audio-Visual Database of Emotional Speech and Song (Ravdess)\n* **Overall Accuracy of this notebook** : 62.407 %","metadata":{}},{"cell_type":"markdown","source":"### 1. Importing Libraries","metadata":{}},{"cell_type":"code","source":"import librosa\nimport librosa.display\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom matplotlib.pyplot import specgram\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport keras\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\nfrom keras.utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import regularizers\nimport os\nimport glob \nimport pandas as pd\nimport IPython.display as ipd\nimport plotly.express as px\nimport scipy.io.wavfile\nimport sys\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-12T05:12:22.329172Z","iopub.execute_input":"2024-04-12T05:12:22.329579Z","iopub.status.idle":"2024-04-12T05:12:22.541108Z","shell.execute_reply.started":"2024-04-12T05:12:22.329547Z","shell.execute_reply":"2024-04-12T05:12:22.540122Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReduceLROnPlateau\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _tf_keras\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/_tf_keras/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tf_keras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/_tf_keras/keras/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m callbacks\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/activations/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deserialize\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialize\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/activations/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m elu\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m exponential\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gelu\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/activations/activations.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/backend/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m result_type\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasTensor\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m any_symbolic_tensors\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_keras_tensor\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/backend/common/keras_tensor.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tree\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaming\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m auto_name\n\u001b[1;32m      6\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.KerasTensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mKerasTensor\u001b[39;00m:\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/utils/tree.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabc\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moptree\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optree'"],"ename":"ModuleNotFoundError","evalue":"No module named 'optree'","output_type":"error"}]},{"cell_type":"markdown","source":"### 2. Preprocessing","metadata":{}},{"cell_type":"markdown","source":"#### 2.1 About Dataset :\n\n**RAVDESS** is one of the most common dataset used for this excercise by others. It's well liked because of its quality of speakers, recording and it has 24 actors of different genders. And there's more! You can get it in song format as well. There's something for everyone and their research project. So for convenience, here's the filename identifiers as per the official RAVDESS website:\n\n* Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n* Vocal channel (01 = speech, 02 = song).\n* Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n* Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n* Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n* Repetition (01 = 1st repetition, 02 = 2nd repetition).\n* Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n\nSo, here's an example of an audio filename. 02-01-06-01-02-01-12.mp4\n\nThis means the meta data for the audio file is:\n\n* Video-only (02)\n* Speech (01)\n* Fearful (06)\n* Normal intensity (01)\n* Statement \"dogs\" (02)\n* 1st Repetition (01)\n* 12th Actor (12) - Female (as the actor ID number is even)","metadata":{}},{"cell_type":"code","source":"RAV = '/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/'\ndir_list = os.listdir(RAV)\n\nemotion = []\ngender = []\npath = []\nfeature = []\nfor i in dir_list:\n    fname = os.listdir(RAV + i)\n    for f in fname:\n        part = f.split('.')[0].split('-')\n        emotion.append(int(part[2]))\n        temp = int(part[6])\n        if temp%2 == 0:\n            temp = \"female\"\n        else:\n            temp = \"male\"\n        gender.append(temp)\n        path.append(RAV + i + '/' + f)\n        \nRAV_df = pd.DataFrame(emotion)\nRAV_df = RAV_df.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\nRAV_df = pd.concat([pd.DataFrame(gender),RAV_df],axis=1)\nRAV_df.columns = ['gender','emotion']\nRAV_df['labels'] =RAV_df.gender + '_' + RAV_df.emotion\nRAV_df['source'] = 'RAVDESS'\nRAV_df = pd.concat([RAV_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nRAV_df = RAV_df.drop(['gender'], axis=1)\nRAV_df.labels.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.070281Z","iopub.status.idle":"2024-04-12T05:03:48.070598Z","shell.execute_reply.started":"2024-04-12T05:03:48.070446Z","shell.execute_reply":"2024-04-12T05:03:48.070461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(RAV_df.head())\ndisplay(RAV_df.describe())","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.071722Z","iopub.status.idle":"2024-04-12T05:03:48.072034Z","shell.execute_reply.started":"2024-04-12T05:03:48.071883Z","shell.execute_reply":"2024-04-12T05:03:48.071899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Data Visualization :","metadata":{}},{"cell_type":"code","source":"px_fig = px.histogram(RAV_df, x='emotion', color='emotion', marginal='box',  \n                      title='Emotion Count')\npx_fig.update_layout(bargap=0.2)\npx_fig.show()\n\npx_fig = px.histogram(RAV_df, x='labels', color='emotion', marginal='box',  \n                      title='Label Count')\npx_fig.update_layout(bargap=0.2)\npx_fig.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.072952Z","iopub.status.idle":"2024-04-12T05:03:48.073271Z","shell.execute_reply.started":"2024-04-12T05:03:48.073120Z","shell.execute_reply":"2024-04-12T05:03:48.073136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_waveplot(data, sr, e):\n    plt.figure(figsize=(10, 3))\n    plt.title('Waveplot for audio with {} emotion'.format(e), size=15)\n    librosa.display.waveshow(data, sr=sr)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.073993Z","iopub.status.idle":"2024-04-12T05:03:48.074460Z","shell.execute_reply.started":"2024-04-12T05:03:48.074288Z","shell.execute_reply":"2024-04-12T05:03:48.074306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_spectrogram(data, sr, e):\n    # stft function converts the data into short term fourier transform\n    X = librosa.stft(data)\n    Xdb = librosa.amplitude_to_db(abs(X))\n    plt.figure(figsize=(12, 3))\n    plt.title('Spectrogram for audio with {} emotion'.format(e), size=15)\n    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')   \n    #librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')\n    plt.colorbar()","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.075384Z","iopub.status.idle":"2024-04-12T05:03:48.075705Z","shell.execute_reply.started":"2024-04-12T05:03:48.075534Z","shell.execute_reply":"2024-04-12T05:03:48.075550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion='fear'\npath = np.array(RAV_df.path[RAV_df.emotion==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nipd.Audio(path)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.076856Z","iopub.status.idle":"2024-04-12T05:03:48.077186Z","shell.execute_reply.started":"2024-04-12T05:03:48.077010Z","shell.execute_reply":"2024-04-12T05:03:48.077026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Data Augmentation :","metadata":{}},{"cell_type":"code","source":"def noise(data):\n    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n\ndef stretch(data, rate=0.8):\n    return librosa.effects.time_stretch(data, rate = rate)\n\ndef shift(data):\n    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n    return np.roll(data, shift_range)\n\ndef pitch(data, sampling_rate, pitch_factor=0.7):\n    return librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=pitch_factor)\n\n# taking any example and checking for techniques.\npath = np.array(RAV_df.path)[1]\ndata, sample_rate = librosa.load(path)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.078394Z","iopub.status.idle":"2024-04-12T05:03:48.078698Z","shell.execute_reply.started":"2024-04-12T05:03:48.078545Z","shell.execute_reply":"2024-04-12T05:03:48.078560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.1 Simple Audio:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(14,4))\nlibrosa.display.waveshow(y=data, sr=sample_rate)\nipd.Audio(path)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.079423Z","iopub.status.idle":"2024-04-12T05:03:48.079719Z","shell.execute_reply.started":"2024-04-12T05:03:48.079573Z","shell.execute_reply":"2024-04-12T05:03:48.079588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.2 Noise Injection :","metadata":{}},{"cell_type":"code","source":"x = noise(data)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveshow(y=x, sr=sample_rate)\nipd.Audio(x, rate=sample_rate)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.080486Z","iopub.status.idle":"2024-04-12T05:03:48.080779Z","shell.execute_reply.started":"2024-04-12T05:03:48.080633Z","shell.execute_reply":"2024-04-12T05:03:48.080648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.3 Stretching :","metadata":{}},{"cell_type":"code","source":"x = stretch(data)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveshow(y=x, sr=sample_rate)\nipd.Audio(x, rate=sample_rate)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.081604Z","iopub.status.idle":"2024-04-12T05:03:48.081900Z","shell.execute_reply.started":"2024-04-12T05:03:48.081752Z","shell.execute_reply":"2024-04-12T05:03:48.081768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.4 Shifting :","metadata":{}},{"cell_type":"code","source":"x = shift(data)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveshow(y=x, sr=sample_rate)\nipd.Audio(x, rate=sample_rate)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.082789Z","iopub.status.idle":"2024-04-12T05:03:48.083099Z","shell.execute_reply.started":"2024-04-12T05:03:48.082937Z","shell.execute_reply":"2024-04-12T05:03:48.082953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.5 Pitch :","metadata":{}},{"cell_type":"code","source":"x = pitch(data,sample_rate)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveshow(y=x, sr=sample_rate)\nipd.Audio(x, rate=sample_rate)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.084016Z","iopub.status.idle":"2024-04-12T05:03:48.084370Z","shell.execute_reply.started":"2024-04-12T05:03:48.084200Z","shell.execute_reply":"2024-04-12T05:03:48.084228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5 Feature Extraction :","metadata":{}},{"cell_type":"code","source":"def extract_features(data):\n    # ZCR\n    result = np.array([])\n    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n    result=np.hstack((result, zcr)) # stacking horizontally\n\n    # Chroma_stft\n    stft = np.abs(librosa.stft(data))\n    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n    result = np.hstack((result, chroma_stft)) # stacking horizontally\n\n    # MFCC\n    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n    result = np.hstack((result, mfcc)) # stacking horizontally\n\n    # Root Mean Square Value\n    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n    result = np.hstack((result, rms)) # stacking horizontally\n\n    # MelSpectogram\n    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n    result = np.hstack((result, mel)) # stacking horizontally\n    \n    return result\n\ndef get_features(path):\n    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n    \n    # without augmentation\n    res1 = extract_features(data)\n    result = np.array(res1)\n    \n    # data with noise\n    noise_data = noise(data)\n    res2 = extract_features(noise_data)\n    result = np.vstack((result, res2)) # stacking vertically\n    \n    # data with stretching and pitching\n    new_data = stretch(data)\n    data_stretch_pitch = pitch(new_data, sample_rate)\n    res3 = extract_features(data_stretch_pitch)\n    result = np.vstack((result, res3)) # stacking vertically\n    \n    return result","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.085448Z","iopub.status.idle":"2024-04-12T05:03:48.085757Z","shell.execute_reply.started":"2024-04-12T05:03:48.085603Z","shell.execute_reply":"2024-04-12T05:03:48.085619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6. Data Preperation","metadata":{}},{"cell_type":"code","source":"X, Y = [], []\nfor path, emotion in zip(RAV_df.path, RAV_df.emotion):\n    feature = get_features(path)\n    for ele in feature:\n        X.append(ele)\n        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n        Y.append(emotion)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.086540Z","iopub.status.idle":"2024-04-12T05:03:48.086865Z","shell.execute_reply.started":"2024-04-12T05:03:48.086697Z","shell.execute_reply":"2024-04-12T05:03:48.086713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X), len(Y), RAV_df.path.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.087670Z","iopub.status.idle":"2024-04-12T05:03:48.087976Z","shell.execute_reply.started":"2024-04-12T05:03:48.087824Z","shell.execute_reply":"2024-04-12T05:03:48.087839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Features = pd.DataFrame(X)\nFeatures['labels'] = Y\nFeatures.to_csv('features.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.088718Z","iopub.status.idle":"2024-04-12T05:03:48.089021Z","shell.execute_reply.started":"2024-04-12T05:03:48.088873Z","shell.execute_reply":"2024-04-12T05:03:48.088889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(Features.head())\ndisplay(Features.describe())","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.090134Z","iopub.status.idle":"2024-04-12T05:03:48.090463Z","shell.execute_reply.started":"2024-04-12T05:03:48.090296Z","shell.execute_reply":"2024-04-12T05:03:48.090311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = Features.iloc[: ,:-1].values\nY = Features['labels'].values","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.091278Z","iopub.status.idle":"2024-04-12T05:03:48.091579Z","shell.execute_reply.started":"2024-04-12T05:03:48.091433Z","shell.execute_reply":"2024-04-12T05:03:48.091448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = OneHotEncoder()\nY = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.092621Z","iopub.status.idle":"2024-04-12T05:03:48.092932Z","shell.execute_reply.started":"2024-04-12T05:03:48.092773Z","shell.execute_reply":"2024-04-12T05:03:48.092789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.093824Z","iopub.status.idle":"2024-04-12T05:03:48.094137Z","shell.execute_reply.started":"2024-04-12T05:03:48.093974Z","shell.execute_reply":"2024-04-12T05:03:48.093990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.094967Z","iopub.status.idle":"2024-04-12T05:03:48.095284Z","shell.execute_reply.started":"2024-04-12T05:03:48.095136Z","shell.execute_reply":"2024-04-12T05:03:48.095152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = np.expand_dims(x_train, axis=2)\nx_test = np.expand_dims(x_test, axis=2)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.096493Z","iopub.status.idle":"2024-04-12T05:03:48.096804Z","shell.execute_reply.started":"2024-04-12T05:03:48.096650Z","shell.execute_reply":"2024-04-12T05:03:48.096666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7. Modelling :","metadata":{}},{"cell_type":"code","source":"model=Sequential()\nmodel.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\nmodel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\nmodel.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\nmodel.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv1D(32, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\nmodel.add(Flatten())\nmodel.add(Dense(units=16, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(units=7, activation='softmax'))\nmodel.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.098136Z","iopub.status.idle":"2024-04-12T05:03:48.098445Z","shell.execute_reply.started":"2024-04-12T05:03:48.098287Z","shell.execute_reply":"2024-04-12T05:03:48.098302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 8. Training :","metadata":{}},{"cell_type":"code","source":"rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=4, min_lr=0.0000001)\nhistory=model.fit(x_train, y_train, batch_size=64, epochs=50, validation_data=(x_test, y_test), callbacks=[rlrp])","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.099374Z","iopub.status.idle":"2024-04-12T05:03:48.099684Z","shell.execute_reply.started":"2024-04-12T05:03:48.099521Z","shell.execute_reply":"2024-04-12T05:03:48.099535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 9. Evaluation :","metadata":{}},{"cell_type":"code","source":"print(\"Accuracy of our model on test data : \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")\n\nplt.style.use('seaborn-darkgrid')\nplt.rcParams.update({'font.size': 12})\nepochs = [i for i in range(50)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_accuracy']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss',marker='o', linewidth=2)\nax[0].plot(epochs , test_loss , label = 'Testing Loss',marker='.', linewidth=2)\nax[0].set_title('Training & Testing Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy',marker='o', linewidth=2)\nax[1].plot(epochs , test_acc , label = 'Testing Accuracy',marker='.', linewidth=2)\nax[1].set_title('Training & Testing Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\n\nplt.subplots_adjust(wspace=0.3)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.100588Z","iopub.status.idle":"2024-04-12T05:03:48.100884Z","shell.execute_reply.started":"2024-04-12T05:03:48.100737Z","shell.execute_reply":"2024-04-12T05:03:48.100752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_test = model.predict(x_test)\ny_pred = encoder.inverse_transform(pred_test)\n\ny_test = encoder.inverse_transform(y_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.101622Z","iopub.status.idle":"2024-04-12T05:03:48.101942Z","shell.execute_reply.started":"2024-04-12T05:03:48.101789Z","shell.execute_reply":"2024-04-12T05:03:48.101804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf['Predicted Labels'] = y_pred.flatten()\ndf['Actual Labels'] = y_test.flatten()\n\ndisplay(df.head())","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.102674Z","iopub.status.idle":"2024-04-12T05:03:48.102962Z","shell.execute_reply.started":"2024-04-12T05:03:48.102818Z","shell.execute_reply":"2024-04-12T05:03:48.102833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize = (12, 10))\ncm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\nsns.heatmap(cm, linecolor='white', cmap='Purples', linewidth=1, annot=True, fmt='')\nplt.title('Confusion Matrix', size=20)\nplt.xlabel('Predicted Labels', size=14)\nplt.ylabel('Actual Labels', size=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.103803Z","iopub.status.idle":"2024-04-12T05:03:48.104125Z","shell.execute_reply.started":"2024-04-12T05:03:48.103961Z","shell.execute_reply":"2024-04-12T05:03:48.103977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-04-12T05:03:48.105383Z","iopub.status.idle":"2024-04-12T05:03:48.105698Z","shell.execute_reply.started":"2024-04-12T05:03:48.105539Z","shell.execute_reply":"2024-04-12T05:03:48.105555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}